from __future__ import annotations

from typing import Any, Dict, Optional, Tuple
import logging
import json
import asyncio

from abc import ABC
from aiokafka import AIOKafkaProducer
from aiokafka.helpers import create_ssl_context


class KafkaOutput(ABC):
    """
    Kafka output implementation using aiokafka + asyncio.Queue.

    Config keys (close to your MQTT config; extras are optional tuning/security):
      - addr: "kafka://host:port"
      - topic: str
      - client_id: str (optional; auto-generated by aiokafka if omitted)
      - queue_max_len: int (default 100)

      # Kafka producer knobs (all optional)
      - acks: 0 | 1 | "all" (default "all")
      - linger_ms: int (default 0)
      - compression_type: "gzip" | "snappy" | "lz4" | "zstd" | None
      - max_request_size: int

      # Security (optional)
      - security_protocol: "PLAINTEXT" | "SASL_PLAINTEXT" | "SSL" | "SASL_SSL"
      - sasl_mechanism: "PLAIN" | "SCRAM-SHA-256" | "SCRAM-SHA-512" | "OAUTHBEARER"
      - username / password: for SASL mechanisms that need them
      - ssl_cafile / ssl_certfile / ssl_keyfile: for SSL/TLS
    """

    def __init__(self, config: Dict[str, Any]):
        super().__init__()
        self.addr: str = config["addr"]
        proto, rest = self.addr.split("://", 1)
        if proto != "kafka":
            raise ValueError(f"addr must start with kafka://, got {proto}://")

        host, port = rest.split(":")
        self.bootstrap_servers = f"{host}:{int(port)}"

        self.topic: str = config["topic"]
        self.client_id: Optional[str] = config.get("client_id")

        # Queue & task state
        self.queue: asyncio.Queue = asyncio.Queue(
            maxsize=int(config.get("queue_max_len", 100))
        )
        self.is_running: bool = False
        self._producer_task: Optional[asyncio.Task] = None

        # Kafka producer config
        self.acks = config.get("acks", "all")
        self.linger_ms = config.get("linger_ms", 0)
        self.compression_type = config.get("compression_type")
        self.max_request_size = config.get("max_request_size", 1048576)

        # Security
        self.security_protocol = config.get("security_protocol", 'PLAINTEXT')
        self.sasl_mechanism = config.get("sasl_mechanism")
        self.username = config.get("username")
        self.password = config.get("password")
        self.ssl_cafile = config.get("ssl_cafile")
        self.ssl_certfile = config.get("ssl_certfile")
        self.ssl_keyfile = config.get("ssl_keyfile")

        self._producer: Optional[AIOKafkaProducer] = None

    #
    # ───────────────────────────────  PUBLIC API  ────────────────────────────────
    #
    async def initialize(self) -> bool:
        """Create and start the aiokafka producer, then launch the background sender."""
        try:
            logging.info(f"Connecting to Kafka at {self.bootstrap_servers}")

            ssl_context = None
            if (self.security_protocol or "").upper() in {"SSL", "SASL_SSL"}:
                ssl_context = create_ssl_context(
                    cafile=self.ssl_cafile,
                    certfile=self.ssl_certfile,
                    keyfile=self.ssl_keyfile,
                )

            producer = AIOKafkaProducer(
                bootstrap_servers=self.bootstrap_servers,
                client_id=self.client_id,
                acks=self.acks,
                linger_ms=self.linger_ms,
                compression_type=self.compression_type,
                max_request_size=self.max_request_size,
                security_protocol=self.security_protocol,
                sasl_mechanism=self.sasl_mechanism,
                sasl_plain_username=self.username,
                sasl_plain_password=self.password,
                ssl_context=ssl_context,
            )
            await producer.start()
            self._producer = producer

            self.is_running = True
            self._producer_task = asyncio.create_task(self._output_producer())

            logging.info("Kafka output initialised")
            return True

        except Exception as e:
            logging.error(f"Failed to initialise Kafka output: {e}", exc_info=True)
            # best-effort cleanup if partially started
            try:
                if self._producer:
                    await self._producer.stop()
            except Exception:
                pass
            self._producer = None
            self.is_running = False
            return False

    #
    # ────────────────────────────────  PRODUCER  ────────────────────────────────
    #
    async def _output_producer(self) -> None:
        """
        Background task:
        • Waits for dict payloads on self.queue
        • JSON-encodes to UTF-8 bytes
        • Sends via aiokafka (await send completion)
        """
        assert self._producer is not None

        while self.is_running:
            try:
                item = await asyncio.wait_for(self.queue.get(), timeout=1.0)

                value_bytes, key, headers, partition = self._serialize(item)

                # Send and await the broker ack according to `acks`
                md = await self._producer.send_and_wait(
                    topic=self.topic,
                    value=value_bytes,
                    key=key,
                    headers=headers,
                    partition=partition,
                )
                # md: RecordMetadata(topic, partition, offset, timestamp, timestamp_type, log_start_offset, checksum, serialized_key_size, serialized_value_size, serialized_header_size)
                logging.debug(
                    "Published to %s p%d@o%d: %s",
                    md.topic, md.partition, md.offset, item
                )
                self.queue.task_done()

            except asyncio.TimeoutError:
                # idle loop – nothing to send right now
                continue
            except Exception as e:
                logging.error(f"Unexpected error in Kafka output producer: {e}", exc_info=True)

    def _serialize(self, item: Any) -> Tuple[bytes, Optional[bytes], Optional[list[tuple[str, bytes]]], Optional[int]]:
        """
        Convert the queued item into Kafka (value, key, headers, partition).

        By default:
          - value = JSON-encoded UTF-8 of `item`
          - key = None
          - headers = None
          - partition = None

        If you want to control key/headers/partition per message without changing the API,
        you can enqueue a dict with optional special fields:
          __kafka_key__ (str|bytes), __kafka_headers__ (dict[str, str|bytes]),
          __kafka_partition__ (int), and the actual payload under "__payload__".
        """
        key = None
        headers = None
        partition = None
        payload = item

        if isinstance(item, dict) and ("__payload__" in item or "__kafka_key__" in item or "__kafka_headers__" in item or "__kafka_partition__" in item):
            payload = item.get("__payload__", {k: v for k, v in item.items() if not k.startswith("__kafka_")})
            k = item.get("__kafka_key__")
            if isinstance(k, str):
                key = k.encode("utf-8")
            elif isinstance(k, (bytes, bytearray)):
                key = bytes(k)

            h = item.get("__kafka_headers__")
            if isinstance(h, dict):
                headers = []
                for hk, hv in h.items():
                    if isinstance(hv, str):
                        headers.append((hk, hv.encode("utf-8")))
                    elif isinstance(hv, (bytes, bytearray)):
                        headers.append((hk, bytes(hv)))
                    elif hv is None:
                        headers.append((hk, b""))
                    else:
                        headers.append((hk, json.dumps(hv, default=str).encode("utf-8")))

            p = item.get("__kafka_partition__")
            if isinstance(p, int):
                partition = p

        # Value serialization
        if isinstance(payload, (bytes, bytearray)):
            value_bytes = bytes(payload)
        else:
            value_bytes = json.dumps(payload, default=str).encode("utf-8")

        return value_bytes, key, headers, partition

    #
    # ────────────────────────────────  WRITE DATA  ────────────────────────────────
    #
    async def write_data(self, results: Dict[str, Any]) -> bool:
        """Put a result dict into the outbound queue."""
        if not self.is_running:
            raise RuntimeError("Kafka output not initialised")

        try:
            await self.queue.put(results)
            return True
        except Exception as e:
            logging.error(f"Failed to queue data: {e}")
            raise RuntimeError(f"Failed to write Kafka data: {e}") from e

    #
    # ────────────────────────────────  CLEAN-UP  ────────────────────────────────
    #
    async def cleanup(self) -> None:
        """Stop producer task, flush, and stop the aiokafka producer."""
        self.is_running = False

        # Stop background task
        if self._producer_task:
            self._producer_task.cancel()
            try:
                await self._producer_task
            except asyncio.CancelledError:
                pass
            self._producer_task = None

        # Flush & stop producer
        if self._producer:
            try:
                await self._producer.flush()
            except Exception:
                # flush best-effort; still proceed to stop
                pass
            try:
                await self._producer.stop()
            except Exception as e:
                logging.error(f"Error stopping Kafka producer: {e}")
            self._producer = None

        # Drain queue
        while not self.queue.empty():
            try:
                self.queue.get_nowait()
                self.queue.task_done()
            except asyncio.QueueEmpty:
                break

        logging.info("Kafka output cleaned up")
